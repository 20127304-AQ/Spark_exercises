{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Java and Spark on Hadoop"
      ],
      "metadata": {
        "id": "epfrvqOy0XbK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LF2fsIkdNPNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf3zOXQkRf0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df774653-4c8b-4e33-e94e-46b4360dfd4e"
      },
      "source": [
        "# install java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,681 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,158 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,046 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,573 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,343 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,213 kB]\n",
            "Fetched 12.4 MB in 7s (1,821 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llc1FhGNQ3U5"
      },
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExFt_N8-z2m",
        "outputId": "415cdee3-8674-414d-fee5-10cb28d96dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a SparkSession in Python"
      ],
      "metadata": {
        "id": "DfAmwnpt1G5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\")\\\n",
        "          .appName(\"Introduction to Spark\")\\\n",
        "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
        "          .getOrCreate()"
      ],
      "metadata": {
        "id": "ZU-oJLNVQl45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B0xdV4gLh8b"
      },
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col, column, expr\n",
        "from pyspark.sql import functions as f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer the questions"
      ],
      "metadata": {
        "id": "MPpD2OQ11RV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0- Load the data files"
      ],
      "metadata": {
        "id": "xdO2c34d1WWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/20127304-AQ/Spark_exercises.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scvJmCk73r8q",
        "outputId": "311e2b7f-6062-4743-c8a7-7b6914b3bdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spark_exercises'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), 815.55 KiB | 1.53 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kNSEFq9TTmP"
      },
      "source": [
        "df_iris = spark.read.csv(\"Spark_exercises/Data/iris.csv\", header = True ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_iris.show(10, truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDOiwgLBDF5L",
        "outputId": "7542c38e-3a75-4973-f578-7cce968d5f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
            "|  6|          5.4|         3.9|          1.7|         0.4|Iris-setosa|\n",
            "|  7|          4.6|         3.4|          1.4|         0.3|Iris-setosa|\n",
            "|  8|          5.0|         3.4|          1.5|         0.2|Iris-setosa|\n",
            "|  9|          4.4|         2.9|          1.4|         0.2|Iris-setosa|\n",
            "| 10|          4.9|         3.1|          1.5|         0.1|Iris-setosa|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Cluster the given examples by using k-means clustering with k = 2, 3, and 5.\n",
        "from pyspark.ml.clustering import *\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml import Pipeline\n",
        "columns = [\n",
        "    'SepalLengthCm',\n",
        "    'SepalWidthCm',\n",
        "    'PetalLengthCm',\n",
        "    'PetalWidthCm'\n",
        "]\n",
        "indexed_cols = [c+\"_index\" for c in columns]\n",
        "\n",
        "indexer = StringIndexer(\n",
        "    inputCols = columns,\n",
        "    outputCols = indexed_cols\n",
        ")\n",
        "\n",
        "vectorizer = VectorAssembler(\n",
        "    inputCols = indexed_cols,\n",
        "    outputCol='features'\n",
        ")\n",
        "\n",
        "def clustering(data, nCluster):\n",
        "  kmeans = KMeans(\n",
        "      k = nCluster\n",
        "  )\n",
        "\n",
        "  pipeline = Pipeline(stages = [indexer, vectorizer, kmeans]).fit(data)\n",
        "  prediction = pipeline.transform(data)\n",
        "  return prediction\n",
        "\n",
        "predictions = []\n",
        "for nCluster in [2, 3, 5]:\n",
        "  predictions.append(clustering(df_iris, nCluster))\n",
        "  predictions[-1].select('features', 'prediction').show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD8VmQww5oCT",
        "outputId": "018a6458-9cac-4aae-afe4-2c95a34028a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----------+\n",
            "|          features|prediction|\n",
            "+------------------+----------+\n",
            "| [1.0,9.0,1.0,0.0]|         1|\n",
            "| [8.0,0.0,1.0,0.0]|         1|\n",
            "|[24.0,2.0,4.0,0.0]|         1|\n",
            "+------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------------------+----------+\n",
            "|          features|prediction|\n",
            "+------------------+----------+\n",
            "| [1.0,9.0,1.0,0.0]|         1|\n",
            "| [8.0,0.0,1.0,0.0]|         1|\n",
            "|[24.0,2.0,4.0,0.0]|         1|\n",
            "+------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n",
            "+------------------+----------+\n",
            "|          features|prediction|\n",
            "+------------------+----------+\n",
            "| [1.0,9.0,1.0,0.0]|         1|\n",
            "| [8.0,0.0,1.0,0.0]|         0|\n",
            "|[24.0,2.0,4.0,0.0]|         0|\n",
            "+------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Consider the clustering with k = 2 done above. For each cluster, count the number of examples that belong to each of the three species. \n",
        "def countClusterSamples(data):\n",
        "  return data.groupBy('prediction').count().orderBy('prediction')\n",
        "\n",
        "countClusterSamples(predictions[0]).show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AuLFPzB9LR5",
        "outputId": "2ea7bd92-3277-4169-9bc2-9f295a1ae380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   47|\n",
            "|         1|  103|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Repeat the counting above for other values of k.\n",
        "for p in predictions:\n",
        "  countClusterSamples(p).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECqhi2Tl9Lfg",
        "outputId": "52af13d2-c61d-4054-8e28-6b1cfaed46b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   47|\n",
            "|         1|  103|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   32|\n",
            "|         1|   96|\n",
            "|         2|   22|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----+\n",
            "|prediction|count|\n",
            "+----------+-----+\n",
            "|         0|   39|\n",
            "|         1|   64|\n",
            "|         2|   20|\n",
            "|         3|    6|\n",
            "|         4|   21|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}