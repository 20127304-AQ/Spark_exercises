{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Java and Spark on Hadoop"
      ],
      "metadata": {
        "id": "epfrvqOy0XbK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LF2fsIkdNPNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf3zOXQkRf0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7152e67c-de20-4240-91e4-dab460c238d8"
      },
      "source": [
        "# install java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,681 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,343 kB]\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,158 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,573 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,046 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,213 kB]\n",
            "Fetched 12.4 MB in 7s (1,740 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llc1FhGNQ3U5"
      },
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ExFt_N8-z2m",
        "outputId": "f055f862-4606-4ab7-a329-6dd5afbef2fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a SparkSession in Python"
      ],
      "metadata": {
        "id": "DfAmwnpt1G5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local\")\\\n",
        "          .appName(\"Introduction to Spark\")\\\n",
        "          .config(\"spark.some.config.option\", \"some-value\")\\\n",
        "          .getOrCreate()"
      ],
      "metadata": {
        "id": "ZU-oJLNVQl45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B0xdV4gLh8b"
      },
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql.functions import col, column, expr\n",
        "from pyspark.sql import functions as f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer the questions"
      ],
      "metadata": {
        "id": "MPpD2OQ11RV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "0- Load the data files"
      ],
      "metadata": {
        "id": "xdO2c34d1WWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/20127304-AQ/Spark_exercises.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scvJmCk73r8q",
        "outputId": "712c8339-d48e-4fa5-d612-715acfbfaec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Spark_exercises'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 10 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), 815.55 KiB | 1.96 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kNSEFq9TTmP"
      },
      "source": [
        "df_mushrooms = spark.read.csv(\"Spark_exercises/Data/mushrooms.csv\", header = True ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mushrooms.show(10, truncate=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDOiwgLBDF5L",
        "outputId": "bec36219-f17c-43cc-94c0-aa7fe70c0ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
            "|class|cap-shape|cap-surface|cap-color|bruises|odor|gill-attachment|gill-spacing|gill-size|gill-color|stalk-shape|stalk-root|stalk-surface-above-ring|stalk-surface-below-ring|stalk-color-above-ring|stalk-color-below-ring|veil-type|veil-color|ring-number|ring-type|spore-print-color|population|habitat|\n",
            "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
            "|    p|        x|          s|        n|      t|   p|              f|           c|        n|         k|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
            "|    e|        x|          s|        y|      t|   a|              f|           c|        b|         k|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      g|\n",
            "|    e|        b|          s|        w|      t|   l|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      m|\n",
            "|    p|        x|          y|        w|      t|   p|              f|           c|        n|         n|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
            "|    e|        x|          s|        g|      f|   n|              f|           w|        b|         k|          t|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        e|                n|         a|      g|\n",
            "|    e|        x|          y|        y|      t|   a|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         n|      g|\n",
            "|    e|        b|          s|        w|      t|   a|              f|           c|        b|         g|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         n|      m|\n",
            "|    e|        b|          y|        w|      t|   l|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         s|      m|\n",
            "|    p|        x|          y|        w|      t|   p|              f|           c|        n|         p|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         v|      g|\n",
            "|    e|        b|          s|        y|      t|   a|              f|           c|        b|         g|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      m|\n",
            "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Prepare the train and test sets following the ratio 80:20\n",
        "# 2. Build a decision tree model on the training set\n",
        "# 3. Build a random forest model on the training set\n",
        "# 4. Evaluate the two models on the same test set\n",
        "# 5. Use a pipeline to simultaneously conduct the above experiments.\n",
        "\n",
        "from pyspark.ml.classification import *\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "def evaluate(pred, metric):\n",
        "  evaluator = MulticlassClassificationEvaluator(\n",
        "      predictionCol = 'prediction',\n",
        "      labelCol = 'class_index',\n",
        "      metricName = metric\n",
        "  )\n",
        "  return round(evaluator.evaluate(pred), 2)\n",
        "\n",
        "train, test = df_mushrooms.randomSplit([.8, .2])\n",
        "\n",
        "columns = df_mushrooms.columns\n",
        "indexed_col = [c+\"_index\" for c in columns]\n",
        "features_col = indexed_col.copy()\n",
        "features_col.remove(\"class_index\")\n",
        "\n",
        "indexer = StringIndexer(\n",
        "    inputCols = columns,\n",
        "    outputCols = indexed_col\n",
        ")\n",
        "\n",
        "vectorizer = VectorAssembler(\n",
        "    inputCols = features_col, \n",
        "    outputCol = 'features'\n",
        ")\n",
        "\n",
        "model1 = DecisionTreeClassifier(\n",
        "    labelCol ='class_index', featuresCol='features'\n",
        ")\n",
        "\n",
        "model2 = RandomForestClassifier(\n",
        "    labelCol = 'class_index', featuresCol='features'\n",
        ")\n",
        "\n",
        "for model in [model1, model2]:\n",
        "  pipeline = Pipeline(stages=[indexer, vectorizer, model]).fit(train)\n",
        "  prediction = pipeline.transform(test)\n",
        "  print(\"Acc: \", evaluate(prediction, 'accuracy'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD8VmQww5oCT",
        "outputId": "95cd0ce5-11d9-4c20-a0a2-5eeb3ae5a9a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc:  1.0\n",
            "Acc:  1.0\n"
          ]
        }
      ]
    }
  ]
}